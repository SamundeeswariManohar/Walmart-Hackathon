{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems\n",
    "\n",
    "\n",
    "![Title](img/title.jpg)\n",
    "\n",
    "> ## Syed Owais Chishti\n",
    "## Data Scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello every and welome to another tutorial. This week we are going to talk about how to increase sales/purchase of products.\n",
    "\n",
    "## but you might be thinking how AI can help in this?\n",
    "\n",
    "### But do not forget, Artificial Intelligence is Super Man.\n",
    "![man](img/man.jpg)\n",
    "\n",
    "### We can use a technique called \"Recommendations System\" to resolve this particular problem, and further there are many algorithms used as RS (will use RS as short form of Recommendation System through out tutorial). Like, \n",
    "* LightFM\n",
    "* Apriori\n",
    "* Eclat etc.\n",
    "\n",
    "## So, why are we waiting? Let's simply dive into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you still do not have idea how RS works, simply try to understand the meaning of following sentence.\n",
    "\n",
    "> **\"people who bought also bought\"**\n",
    "\n",
    "** above line is the key idea of RS, it helps you in purchasing relevant goods and helps seller so that he can put relevant items at one place**\n",
    "\n",
    "** Got it ? Ok let's take an example, few years ago it was found that two products were sold rapidly in evening timings,**\n",
    "* Baby Diapers.\n",
    "* Beer (beverage).\n",
    "\n",
    "## It maked me shoked !!!\n",
    "![shoc](img/shoc.jpg)\n",
    "\n",
    "** But, the fact is, normally in evenings man went out to purchase baby diapers and due to tiredness they bought beer.**\n",
    "\n",
    "## AI rocks.\n",
    "![rocks](img/rocks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RS\n",
    "\n",
    "* Collaborative.\n",
    "* Content Based.\n",
    "\n",
    "** Let's discuss a bit about types.**\n",
    "\n",
    "### Collaborative:\n",
    "\n",
    "** It predicts your likeness on the basis of other people likenesses due to their purchases in past.**\n",
    "\n",
    "### Content Based.\n",
    "\n",
    "** It predicts your likeness on the basis of your likenesses due to your purchases in past.**\n",
    "\n",
    "### All the big companies like Google (for search engine), Amazon (for products), Youtube (for videos), Facebook (for relevant friends) use this RS technology with both Collaborative and Content Based types to improve user activities.\n",
    "![all](img/all.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now see how RS works by coding it with my love \"PYTHON\".\n",
    "\n",
    "** We are going to use two types of Algorithms,**\n",
    "* LightFM and\n",
    "* Apriori.\n",
    "\n",
    "** with movies and grocery items respectively.**\n",
    "\n",
    "** I am using Anaconda Enviroment with Python 3.6.x., to download anaconda check out my prevoius tutorials.\n",
    "But LightFM library is not available with Anaconda, you need to download its .whl file** [here](https://ci.appveyor.com/api/buildjobs/uj88ko5l53rmqdkt/artifacts/dist%2Flightfm-1.13-cp36-cp36m-win_amd64.whl).\n",
    "\n",
    "** After you installed .whl file go to the directory where it is placed and press \"Shift+Rightclick\" and press \"open command window here\" (This is for 64-bit windows, for MAC open terminal in current directory) and enter**\n",
    "\n",
    "> ** pip install lightfm-1.13-cp36-cp36m-win_amd64.whl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({'chicken', 'light cream'}), 0.004532728969470737, [OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]]\n",
      "[frozenset({'escalope', 'mushroom cream sauce'}), 0.005732568990801226, [OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]]\n",
      "[frozenset({'pasta', 'escalope'}), 0.005865884548726837, [OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]]\n",
      "[frozenset({'honey', 'fromage blanc'}), 0.003332888948140248, [OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]]\n",
      "[frozenset({'ground beef', 'herb & pepper'}), 0.015997866951073192, [OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]]\n",
      "[frozenset({'ground beef', 'tomato sauce'}), 0.005332622317024397, [OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]]\n",
      "[frozenset({'light cream', 'olive oil'}), 0.003199573390214638, [OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]]\n",
      "[frozenset({'whole wheat pasta', 'olive oil'}), 0.007998933475536596, [OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]]\n",
      "[frozenset({'shrimp', 'pasta'}), 0.005065991201173177, [OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)]]\n",
      "[frozenset({'spaghetti', 'milk', 'avocado'}), 0.003332888948140248, [OrderedStatistic(items_base=frozenset({'spaghetti', 'avocado'}), items_add=frozenset({'milk'}), confidence=0.41666666666666663, lift=3.215449245541838)]]\n"
     ]
    }
   ],
   "source": [
    "## Apriori algorithm with grocery data set.\n",
    "\n",
    "# first install necessary libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# read data set\n",
    "dataset = pd.read_csv('Market_Basket_Optimisation.csv', header=None)\n",
    "\n",
    "transactions = []\n",
    "for i in range(0,7501):\n",
    "    transactions.append([str(dataset.values[i,j]) for j in range(0,20)])\n",
    "\n",
    "from apyori import apriori\n",
    "rules = apriori(transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_lenght=2)# min_lenght is to set no. of relevant products\n",
    "\n",
    "results = list(rules)\n",
    "\n",
    "myResults = [list(x) for x in results]\n",
    "\n",
    "for items in range(10):\n",
    "    print(myResults[items])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** [frozenset({'chicken', 'light cream'}), 0.004532728969470737, [OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]] **\n",
    "\n",
    "** In this line you can clearly see that people who bought chicken also bought ice cream (may they used to eat ice cream while waiting during order).**\n",
    "\n",
    "**To understand other parts of answer let's understand few basic parts of apriori**\n",
    "\n",
    "* Support. If we talk about movie recommendations (M), then, support(M) = # of watchlists containing(M) / # of user watchlists.\n",
    "** Assume 10 people saw the movie M out of 100. support(M) = 10/100 = 10%**\n",
    "\n",
    "\n",
    "* Confidence (from movie1 M1 to movie2 M2). confidence(M1-->M2) = # of watchlists of M1 and M2 / # of user watchlists of M1.\n",
    "** Say 80 people saw movie M1 and 7 people saw movie M2 and M1 both. confidence(M1-->M2) = 7/80 = 17.5%**\n",
    "\n",
    "\n",
    "* Lift(M1-->M2) = confidence(M1-->M2) / support(M2).\n",
    "** Lift(M1-->M2) = 17.5 / 10 = 1.75**\n",
    "\n",
    "\n",
    "** Lift gives the ratio of suggestions, the higher the ratio of lift the more suggestions to person for movie2 already seen movie1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **rules = apriori(transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_lenght=2)**\n",
    "\n",
    "** All three parameters of apriori can be tuned according to your requirements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see LightFM Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 49906 stored elements in COOrdinate format>\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n",
      "User 3\n",
      " Watched before\n",
      "     Seven (Se7en) (1995)\n",
      "     Contact (1997)\n",
      "     Starship Troopers (1997)\n",
      " Recommended for user: \n",
      "     Contact (1997)\n",
      "     Scream (1996)\n",
      "     G.I. Jane (1997)\n",
      "User 25\n",
      " Watched before\n",
      "     Dead Man Walking (1995)\n",
      "     Star Wars (1977)\n",
      "     Fargo (1996)\n",
      " Recommended for user: \n",
      "     Contact (1997)\n",
      "     Fargo (1996)\n",
      "     English Patient, The (1996)\n",
      "User 450\n",
      " Watched before\n",
      "     Contact (1997)\n",
      "     George of the Jungle (1997)\n",
      "     Event Horizon (1997)\n",
      " Recommended for user: \n",
      "     G.I. Jane (1997)\n",
      "     Kiss the Girls (1997)\n",
      "     Scream (1996)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm import LightFM\n",
    "\n",
    "data = fetch_movielens(min_rating=4.0) # to avoid space issues using only those movies which are rated 4 or above.\n",
    "\n",
    "print(repr(data['train']))\n",
    "\n",
    "print(repr(data['test']))\n",
    "\n",
    "model = LightFM(loss='warp')\n",
    "\n",
    "model.fit(data['train'], epochs=30, num_threads=2)\n",
    "\n",
    "def RS(model, data,user_ids):\n",
    "    n_users, n_items = data['train'].shape\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        known_positives = data['item_labels'][data['train'].tocsr() [user_id].indices]\n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        top_items = data['item_labels'][np.argsort(-scores)]\n",
    "\n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\" Watched before\")\n",
    "        for x in known_positives[:3]:\n",
    "            print (\"     %s\" % x)\n",
    "        print(\" Recommended for user: \")\n",
    "        for x in top_items[:3]:\n",
    "            print(\"     %s\" % x)\n",
    "RS(model, data, [3,25,450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both Apriori and LightFM are good algorithms to start, but LightFM is better due to it contains several data sets in it. Though both algorithms can give you kick to start using RS in your AI works.\n",
    "\n",
    "** WARP above is \"weighted Approximate-Rank Pairwise\", It helps to predict on the basis of previous other users likes. It uses Gradient descent to iteratively find the weights which improve our predictions over time.**\n",
    "\n",
    "\n",
    "## Topic for next week is **Decentralized Applications (Dapp)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hope you enjoyed this tutorial.\n",
    "\n",
    "\n",
    "## Check my other tutorials as well, and also give your reviews on my research papers.\n",
    "\n",
    "* [More Tutorials on GitHub](https://github.com/elacsoft)\n",
    "* [Research Papers](https://github.com/elacsoft/Research-Papers)\n",
    "\n",
    "## Syed Owais Chishti\n",
    "### Data Scientist.\n",
    "\n",
    ">* [MyWebsite](http://elacsoft.cf) \n",
    "* [MyTwitterAccount](https://twitter.com/robertw26984557)\n",
    "* [MyFacebookAccount](https://www.facebook.com/owais.chishti.35)\n",
    "* [MyGitHubAccount](https://github.com/elacsoft)\n",
    "* [MyYoutubeChannel](https://www.youtube.com/channel/UCWha7dSnT_k7mBGJ0XFB0IQ)\n",
    "* [elacsoft@gmail.com](elacsoft@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
